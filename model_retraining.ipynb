{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5641cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install snowflake-connector-python\n",
    "# ! pip install snowflake-sqlalchemy\n",
    "# ! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77131cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "\n",
    "import sqlalchemy\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine, text\n",
    "from snowflake.sqlalchemy import *\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pytz\n",
    "tz_ny = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "from utils import preprocess_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be70f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the snowflake credits\n",
    "ACCOUNT = \"qq49985.ap-southeast-1\"\n",
    "USERNAME = \"prxdyu\"\n",
    "PASSWORD = \"Gala2471xy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b84a2",
   "metadata": {},
   "source": [
    "### Retraining \n",
    "\n",
    "#### Steps\n",
    "1. Get the complete data from the database except last 2 weeks data\n",
    "2. Process and select the best features using the select_features function\n",
    "3. Build the new model using data from step 1\n",
    "4. Compare the performance of the existing model and new model on the last 2 weeks data\n",
    "5. Pick the best one if new model performs best then replace the existing deployed model file with the new model file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb592578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to utils.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile utils.py -a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"MODEL RETRAINING UTILITY FUNCTIONS\"\"\"\n",
    "\n",
    "# Orders the features in the dataset, if unknown new features are added then it assings 0 for all values in that feature\"\"\"\n",
    "def validate_features(df,features_list):\n",
    "    \n",
    "    test = pd.DataFrame()\n",
    "    for col in features_list:\n",
    "        if col in df.columns.tolist():\n",
    "            test[col]=df[col]\n",
    "        else:\n",
    "            test[col]=0\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dcc73cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to utils.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile utils.py -a\n",
    "\n",
    "\n",
    "# Selects the best features for model building\n",
    "def select_features(df):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    import xgboost as xgb\n",
    "\n",
    "    \n",
    "    # splitting the target and input features\n",
    "    x = df.drop(columns=['LOS'])\n",
    "    y = df[['LOS']]\n",
    "\n",
    "    # train test splitting\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,stratify=y)\n",
    "    \n",
    "\n",
    "    # 1. DECISION TREE\n",
    "    \n",
    "    dtree = DecisionTreeRegressor()\n",
    "    # fitting the tree\n",
    "    dtree.fit(x_train,y_train)\n",
    "    # Checking the feature importance\n",
    "    zipper = dict(zip(x_train.columns,dtree.feature_importances_))\n",
    "    # making it as a dataframe\n",
    "    feature_importances= pd.DataFrame.from_dict(zipper,orient='index').reset_index().rename(columns={\"index\":\"feature\",0:\"importance\"}).sort_values(by='importance',ascending=False)\n",
    "    # taking the features which have importance greater than threshold 0.01\n",
    "    dtree_features = feature_importances[feature_importances['importance']>0.01]['feature'].values.tolist()\n",
    "    \n",
    "    # 2. XGBOOST\n",
    "    \n",
    "    xgb_ = xgb.XGBRegressor()\n",
    "    xgb_.fit(x_train,y_train)\n",
    "    xgb_.score(x_train,y_train)\n",
    "    # Checking the feature importance\n",
    "    zipper = dict(zip(x_train.columns,xgb_.feature_importances_))\n",
    "    # making it as a dataframe\n",
    "    xgb_feature_importances= pd.DataFrame.from_dict(zipper,orient='index').reset_index().rename(columns={\"index\":\"feature\",0:\"importance\"}).sort_values(by='importance',ascending=False)\n",
    "    # taking the features which have importance greater than threshold 0.01\n",
    "    xgb_features = feature_importances[xgb_feature_importances['importance']>0.01]['feature'].values.tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Joining the features from both dtree and xgboost\n",
    "    final_features = list(set(dtree_features).union(set(xgb_features)))\n",
    "    \n",
    "\n",
    "    # exporting the list of final features for future predictions\n",
    "    with open('retraining_artifacts/retrained_final_features.pkl', 'wb') as f:\n",
    "        pickle.dump(final_features, f)\n",
    "        \n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a416e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to utils.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile utils.py -a\n",
    "\n",
    "\n",
    "# defining a function to create query to fetch the data for retraining\n",
    "def get_retraining_query(max_date):\n",
    "    \n",
    "    # defining the query (taking old training data from HEALTH_DATA table and new data from PREDICTION_LOGGING table )\n",
    "    query =f\"\"\"\n",
    "\n",
    "            WITH TRAIN_BASE AS (\n",
    "\n",
    "                SELECT CASE_ID,\n",
    "                       COALESCE(HOSPITAL_CODE,0) AS HOSPITAL_CODE,\n",
    "                       COALESCE(HOSPITAL_TYPE_CODE,'None') AS HOSPITAL_TYPE_CODE,\n",
    "                       COALESCE(CITY_CODE_HOSPITAL,0) AS CITY_CODE_HOSPITAL,\n",
    "                       COALESCE(HOSPITAL_REGION_CODE,'None') AS HOSPITAL_REGION_CODE,\n",
    "                       COALESCE(AVAILABLE_EXTRA_ROOMS_IN_HOSPITAL,0) AS AVAILABLE_EXTRA_ROOMS_IN_HOSPITAL,\n",
    "                       COALESCE(DEPARTMENT,'None') AS DEPARTMENT,\n",
    "                       COALESCE(WARD_TYPE,'None') AS WARD_TYPE,\n",
    "                       COALESCE(WARD_FACILITY_CODE,'None') AS WARD_FACILITY_CODE,\n",
    "                       COALESCE(BED_GRADE,0) AS BED_GRADE,\n",
    "                       PATIENTID,\n",
    "                       COALESCE(CITY_CODE_PATIENT,0) AS CITY_CODE_PATIENT,\n",
    "                       COALESCE(TYPE_OF_ADMISSION,'None') AS TYPE_OF_ADMISSION,\n",
    "                       COALESCE(SEVERITY_OF_ILLNESS,'Minor') AS SEVERITY_OF_ILLNESS,\n",
    "                       COALESCE(VISITORS_WITH_PATIENT,0) AS VISITORS_WITH_PATIENT,\n",
    "                       COALESCE(AGE,'None') AS AGE,\n",
    "                       COALESCE(ADMISSION_DEPOSIT,0) AS ADMISSION_DEPOSIT,\n",
    "                       ADMISSION_DATE,\n",
    "                       DISCHARGE_DATE\n",
    "\n",
    "                FROM HEALTH_DB.PUBLIC.HEALTH_DATA\n",
    "\n",
    "            ),\n",
    "\n",
    "            TRAIN_BASE_WITH_FEATURES AS (\n",
    "\n",
    "                SELECT *,\n",
    "                        MONTHNAME(ADMISSION_DATE) AS ADMISSION_MONTH,\n",
    "                        DAYNAME(ADMISSION_DATE) AS ADMISSION_DAY,    \n",
    "                        CONCAT(TYPE_OF_ADMISSION,'-',SEVERITY_OF_ILLNESS) AS ADMISSION_ILLNESS,\n",
    "                        CONCAT(SEVERITY_OF_ILLNESS,'-',BED_GRADE) AS ILLNESS_BEDGRADE,\n",
    "                        CONCAT(DEPARTMENT,'-',SEVERITY_OF_ILLNESS) AS DEPARTMENT_ILLNESS,\n",
    "                        DATEDIFF(day,ADMISSION_DATE,DISCHARGE_DATE) AS LOS\n",
    "                FROM TRAIN_BASE \n",
    "\n",
    "            ),\n",
    "            \n",
    "            NEW_DATA_WITH_FEATURES AS (\n",
    "            \n",
    "                SELECT CASE_ID,\n",
    "                       COALESCE(HOSPITAL_CODE,0) AS HOSPITAL_CODE,\n",
    "                       COALESCE(HOSPITAL_TYPE_CODE,'None') AS HOSPITAL_TYPE_CODE,\n",
    "                       COALESCE(CITY_CODE_HOSPITAL,0) AS CITY_CODE_HOSPITAL,\n",
    "                       COALESCE(HOSPITAL_REGION_CODE,'None') AS HOSPITAL_REGION_CODE,\n",
    "                       COALESCE(AVAILABLE_EXTRA_ROOMS_IN_HOSPITAL,0) AS AVAILABLE_EXTRA_ROOMS_IN_HOSPITAL,\n",
    "                       COALESCE(DEPARTMENT,'None') AS DEPARTMENT,\n",
    "                       COALESCE(WARD_TYPE,'None') AS WARD_TYPE,\n",
    "                       COALESCE(WARD_FACILITY_CODE,'None') AS WARD_FACILITY_CODE,\n",
    "                       COALESCE(BED_GRADE,0) AS BED_GRADE,\n",
    "                       PATIENTID,\n",
    "                       COALESCE(CITY_CODE_PATIENT,0) AS CITY_CODE_PATIENT,\n",
    "                       COALESCE(TYPE_OF_ADMISSION,'None') AS TYPE_OF_ADMISSION,\n",
    "                       COALESCE(SEVERITY_OF_ILLNESS,'Minor') AS SEVERITY_OF_ILLNESS,\n",
    "                       COALESCE(VISITORS_WITH_PATIENT,0) AS VISITORS_WITH_PATIENT,\n",
    "                       COALESCE(AGE,'None') AS AGE,\n",
    "                       COALESCE(ADMISSION_DEPOSIT,0) AS ADMISSION_DEPOSIT,\n",
    "                       ADMISSION_DATE,\n",
    "                       DISCHARGE_DATE,\n",
    "                       ADMISSION_MONTH,\n",
    "                       ADMISSION_DAY,\n",
    "                       ADMISSION_ILLNESS,\n",
    "                       ILLNESS_BEDGRADE,\n",
    "                       DEPARTMENT_ILLNESS,\n",
    "                       LOS\n",
    "                FROM HEALTH_DB.PUBLIC.PREDICTION_LOGGING\n",
    "                WHERE ADMISSION_DATE<='{max_date}'\n",
    "                \n",
    "            \n",
    "            )\n",
    "            \n",
    "            SELECT * FROM TRAIN_BASE_WITH_FEATURES\n",
    "            UNION ALL\n",
    "            SELECT * FROM NEW_DATA_WITH_FEATURES\n",
    "           \n",
    "            \"\"\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f90ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to utils.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile utils.py -a\n",
    "\n",
    "\n",
    "# defining the function for retraining\n",
    "def retrain_model(cut_off_date):\n",
    "    \n",
    "    # Creating the connection engine (way 1)\n",
    "    engine = create_engine(URL(\n",
    "            account=ACCOUNT,\n",
    "            user= USERNAME,\n",
    "            password= PASSWORD,\n",
    "            role=\"ACCOUNTADMIN\",\n",
    "            warehouse=\"COMPUTE_WH\",\n",
    "            database=\"HEALTH_DB\",\n",
    "            schema=\"PUBLIC\"\n",
    "        ))\n",
    "    \n",
    "    # getting the query \n",
    "    query = get_retraining_query(cut_off_date)\n",
    "    \n",
    "    # connecting to the engine\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        data = pd.DataFrame(result.fetchall())\n",
    "        data.columns = result.keys()\n",
    "        data.columns = [col.upper() for col in data.columns.tolist()]\n",
    "    \n",
    "    print(\"Successfully fetched data from snowflake\\n\")\n",
    "    print(\"Shape of fetched data is \",data.shape)\n",
    "    \n",
    "    # defining the max and min dates for data splitting \n",
    "    max_date = data['ADMISSION_DATE'].max()\n",
    "    min_date = max_date-timedelta(days=7)\n",
    "    \n",
    "    # splitting the data into train and test\n",
    "    d_train = data[data['ADMISSION_DATE']<=min_date]\n",
    "    d_test = data[(data['ADMISSION_DATE']>min_date) & (data['ADMISSION_DATE']<=max_date)]\n",
    "    \n",
    "    print(\"Train, Test split done\\n\")\n",
    "        \n",
    "    # applying the preprocess steps to both train and test\n",
    "    df_train = preprocess_data(d_train)\n",
    "    df_test = preprocess_data(d_test)\n",
    "    \n",
    "    print(\"Preprocessing of Train and test data is done\\n\")\n",
    "    \n",
    "    \n",
    "    # selecting features\n",
    "    final_features = select_features(df_train)\n",
    "    df_test_processed = validate_features(df_test,final_features)\n",
    "    \n",
    "    print(\"Feature selection executed\\n\")\n",
    "\n",
    "    \n",
    "    # Model building\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import root_mean_squared_error,mean_absolute_error\n",
    "    \n",
    "    xgb_ = xgb.XGBRegressor()\n",
    "    xgb_.fit(df_train[final_features],df_train['LOS'])\n",
    "    \n",
    "    # getting the predictions for the test data (last 1 week's data)\n",
    "    y_test_pred = np.ceil(xgb_.predict(df_test_processed))\n",
    "    \n",
    "    # computing the performance metrics\n",
    "    rmse = root_mean_squared_error(y_test_pred,df_test['LOS'])\n",
    "    mae = mean_absolute_error(y_test_pred,df_test['LOS'])\n",
    "    print(\"Test performance of new retrained model \\n\")\n",
    "    print(f\"RMSE is {rmse}\")\n",
    "    print(f\"MAE is {mae}\\n\\n\")\n",
    "    \n",
    "    # storinig the performance metrics for future use\n",
    "    retrained_model_metrics = dict()\n",
    "    retrained_model_metrics['RMSE']=rmse\n",
    "    retrained_model_metrics['MAE']=mae\n",
    "    import pickle\n",
    "    with open('retraining_artifacts/retrained_model_metrics.pkl', 'wb') as f:\n",
    "        pickle.dump(retrained_model_metrics, f)\n",
    "    \n",
    "    # saving the model\n",
    "    booster = xgb_.get_booster()\n",
    "    booster.save_model('retraining_artifacts/xgb_retrained.model')\n",
    "    print(\"Successfully saved the retrained model\\n\")\n",
    "    \n",
    "    # loading the old model\n",
    "    old_model = xgb.XGBRegressor()\n",
    "    old_model.load_model('artifacts/xgb.model')\n",
    "    print(\"Sucessfully loaded old model\")\n",
    "    \n",
    "    # loading the selected features for our old model\n",
    "    with open('artifacts/final_features.pkl','rb') as f:\n",
    "        final_features_old = pickle.load(f)\n",
    "    \n",
    "    # getting predictions the test data (last 1 week's data) from our old model\n",
    "    df_test_processed_old = validate_features(df_test,final_features_old)\n",
    "    y_test_pred_old = np.ceil(old_model.predict(df_test_processed_old))\n",
    "    \n",
    "    # computing the performance metrics\n",
    "    rmse_old = root_mean_squared_error(y_test_pred_old,df_test['LOS'])\n",
    "    mae_old = mean_absolute_error(y_test_pred_old,df_test['LOS'])\n",
    "    print(\"Test performance of old existing model\")\n",
    "    print(f\"RMSE is {rmse_old}\")\n",
    "    print(f\"MAE is {mae_old}\")\n",
    "    \n",
    "    # storinig the performance metrics for future use\n",
    "    old_model_metrics = dict()\n",
    "    old_model_metrics['RMSE']=rmse_old\n",
    "    old_model_metrics['MAE']=mae_old\n",
    "    \n",
    "    return retrained_model_metrics,old_model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667b97b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to utils.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile utils.py -a\n",
    "\n",
    "# defining a function which chooses the model to deploy based on the performance metric\n",
    "def finalize_model(old_model_metrics,new_model_metrics):\n",
    "    count=0\n",
    "    \n",
    "    # checking if the RMSE and MAE of new model metric is lesser than t\n",
    "    for metric in new_model_metrics.keys():\n",
    "        if new_model_metrics[metric]<old_model_metrics[metric]:\n",
    "            count+=1\n",
    "    \n",
    "    if count>0:\n",
    "        return 'New Model '\n",
    "    else:\n",
    "        return 'Old Model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96d5ea",
   "metadata": {},
   "source": [
    "#### Retraining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84523e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched data from snowflake\n",
      "\n",
      "Shape of fetched data is  (243790, 25)\n",
      "Train, Test split done\n",
      "\n",
      "Preprocessing of Train and test data is done\n",
      "\n",
      "Feature selection executed\n",
      "\n",
      "Test performance of new retrained model \n",
      "\n",
      "RMSE is 11.973041923299403\n",
      "MAE is 8.614589530041641\n",
      "\n",
      "\n",
      "Successfully saved the retrained model\n",
      "\n",
      "Sucessfully loaded old model\n",
      "Test performance of old existing model\n",
      "RMSE is 11.859149460941214\n",
      "MAE is 8.501115407495538\n"
     ]
    }
   ],
   "source": [
    "from utils import retrain_model\n",
    "\n",
    "# retraining a new model and getting the performance metrics \n",
    "old_model_metrics,new_model_metrics = retrain_model('2022-12-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25e7d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Model '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finalizing which model to deploy\n",
    "finalize_model(old_model_metrics,new_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3741061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py -a\n",
    "\n",
    "\n",
    "# defining the function to deploy the model\n",
    "def deploy_model(selector=\"Old Model\"):\n",
    "    \n",
    "    if selector!=\"Old Model\":\n",
    "        \n",
    "        # LOADING THE OLD MODEL ARTIFACTS\n",
    "        with open('artifacts/final_features.pkl','rb') as f:\n",
    "            old_model_features = pickle.load(f)\n",
    "        with open('artifacts/model_ref_metrics.pkl','rb') as f:\n",
    "            old_model_metrics = pickle.load(f)\n",
    "        with open('artifacts/xgb.model','rb') as f:\n",
    "            old_model = pickle.load(f)\n",
    "        \n",
    "        \n",
    "        # MOVING THE OLD MODEL ARTIFACTS TO THE ARCHIVE FOLDER\n",
    "        with open('archive/old_model_features.pkl','wb') as f:\n",
    "            pickle.dump(old_model_features,f)\n",
    "        with open('archive/old_model_metrics.pkl','wb') as f:\n",
    "            pickle.dump(old_model_metrics,f)\n",
    "        with open('archive/old_model.model','wb') as f:\n",
    "            pickle.dump(old_model,f)\n",
    "            \n",
    "        # LOADING THE NEW MODEL\n",
    "        with open('retraining_artifacts/retrained_final_features.pkl','rb') as f:\n",
    "            new_model_features = pickle.load(f)\n",
    "        with open('retraining_artifacts/retrained_model_metrics.pkl','rb') as f:\n",
    "            new_model_metrics = pickle.load(f)\n",
    "        with open('retraining_artifacts/xgb_retrained.model','rb') as f:\n",
    "            new_model = pickle.load(f)\n",
    "            \n",
    "        \n",
    "        # REPLACING THE OLD MODEL WITH THE NEW RETRAINED MODEL\n",
    "        with open('artifacts/final_features.pkl','wb') as f:\n",
    "            pickle.dump(new_model_features,f)\n",
    "        with open('artifacts/model_ref_metrics.pkl','wb') as f:\n",
    "            pickle.dump(new_model_metrics,f)\n",
    "        with open('artifacts/xgb.model','wb') as f:\n",
    "            pickle.dump(new_model,f)\n",
    "            \n",
    "        print(\"Deployment New Model Successfully\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Keeping the same Model\")\n",
    "    \n",
    "    return \"Deployment Succesful\"\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e750d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
